import os
import argparse
from typing import Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from peft import PeftModel

# å°è¯•å¯¼å…¥gradioç”¨äºWebç•Œé¢
try:
    import gradio as gr
    HAS_GRADIO = True
except ImportError:
    HAS_GRADIO = False
    print("æç¤º: å®‰è£… gradio å¯å¯ç”¨Webç•Œé¢ (pip install gradio)")


# ==================== æ¨ç†æ¨¡å¼ ====================
REASONING_MODES = {
    "standard": {
        "name": "æ ‡å‡†æ¨¡å¼",
        "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦è€å¸ˆï¼Œè¯·ç›´æ¥è§£ç­”é—®é¢˜ã€‚",
        "prefix": "",
    },
    "cot": {
        "name": "é“¾å¼æ€ç»´ (CoT)",
        "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦è€å¸ˆã€‚è¯·ä¸€æ­¥ä¸€æ­¥åœ°æ€è€ƒï¼Œå±•ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ã€‚",
        "prefix": "è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥æ€è€ƒè¿™ä¸ªé—®é¢˜ï¼š\n\n",
    },
    "detailed": {
        "name": "è¯¦ç»†è§£æ",
        "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦è€å¸ˆã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è§£ç­”ï¼š\n1. ã€é¢˜ç›®åˆ†æã€‘\n2. ã€è§£é¢˜æ€è·¯ã€‘\n3. ã€è®¡ç®—è¿‡ç¨‹ã€‘\n4. ã€ç­”æ¡ˆéªŒè¯ã€‘",
        "prefix": "",
    },
    "simple": {
        "name": "ç®€æ´æ¨¡å¼",
        "system": "ä½ æ˜¯æ•°å­¦è€å¸ˆï¼Œè¯·ç®€æ´åœ°è§£ç­”é—®é¢˜ï¼Œç»™å‡ºå…³é”®æ­¥éª¤å’Œç­”æ¡ˆã€‚",
        "prefix": "",
    },
}


class MathAssistant:
    """æ•°å­¦é—®ç­”åŠ©æ‰‹"""
    
    def __init__(
        self,
        model_path: str,
        lora_path: Optional[str] = None,
        device: str = "cuda",
    ):
        self.device = device
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        if lora_path and os.path.exists(lora_path):
            print(f"åŠ è½½LoRAæƒé‡: {lora_path}")
            self.model = PeftModel.from_pretrained(self.model, lora_path)
        
        self.model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def solve(
        self,
        question: str,
        mode: str = "cot",
        max_new_tokens: int = 512,
        temperature: float = 0.1,
        stream: bool = False,
    ) -> str:
        """è§£ç­”æ•°å­¦é—®é¢˜"""
        if mode not in REASONING_MODES:
            mode = "cot"
        
        config = REASONING_MODES[mode]
        
        messages = [
            {"role": "system", "content": config["system"]},
            {"role": "user", "content": question},
        ]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        
        if config["prefix"]:
            text += config["prefix"]
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
        if stream:
            streamer = TextStreamer(self.tokenizer, skip_prompt=True)
        else:
            streamer = None
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                streamer=streamer,
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True,
        )
        
        if config["prefix"] and not response.startswith(config["prefix"]):
            response = config["prefix"] + response
        
        return response


def run_cli(assistant: MathAssistant):
    """å‘½ä»¤è¡Œäº¤äº’ç•Œé¢"""
    print("\n" + "=" * 60)
    print("å°å­¦æ•°å­¦æ¨ç†é—®ç­”ç³»ç»Ÿ")
    print("=" * 60)
    print("\nå¯ç”¨æ¨¡å¼:")
    for key, value in REASONING_MODES.items():
        print(f"  {key}: {value['name']}")
    print("\nå‘½ä»¤:")
    print("  /mode <æ¨¡å¼å>  - åˆ‡æ¢æ¨ç†æ¨¡å¼")
    print("  /quit          - é€€å‡º")
    print("=" * 60)
    
    current_mode = "cot"
    print(f"\nå½“å‰æ¨¡å¼: {REASONING_MODES[current_mode]['name']}")
    
    while True:
        try:
            user_input = input("\nğŸ“ è¯·è¾“å…¥æ•°å­¦é—®é¢˜: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nå†è§ï¼")
            break
        
        if not user_input:
            continue
        
        if user_input.startswith("/"):
            parts = user_input.split()
            cmd = parts[0].lower()
            
            if cmd == "/quit":
                print("å†è§ï¼")
                break
            elif cmd == "/mode" and len(parts) > 1:
                new_mode = parts[1]
                if new_mode in REASONING_MODES:
                    current_mode = new_mode
                    print(f"âœ… å·²åˆ‡æ¢åˆ°: {REASONING_MODES[current_mode]['name']}")
                else:
                    print(f"âŒ æœªçŸ¥æ¨¡å¼: {new_mode}")
            else:
                print("âŒ æœªçŸ¥å‘½ä»¤")
            continue
        
        print(f"\nğŸ¤” æ€è€ƒä¸­... (æ¨¡å¼: {REASONING_MODES[current_mode]['name']})\n")
        print("-" * 40)
        
        response = assistant.solve(user_input, mode=current_mode, stream=True)
        
        print("-" * 40)


def create_gradio_interface(assistant: MathAssistant):
    """åˆ›å»ºGradio Webç•Œé¢"""
    
    def solve_problem(question, mode, temperature):
        if not question.strip():
            return "è¯·è¾“å…¥æ•°å­¦é—®é¢˜"
        response = assistant.solve(
            question, 
            mode=mode, 
            temperature=temperature
        )
        return response
    
    # æ¨¡å¼é€‰æ‹©åˆ—è¡¨
    mode_choices = [k for k in REASONING_MODES.keys()]
    
    # ä½¿ç”¨ gr.Interface æ›´ç®€æ´ã€å…¼å®¹æ€§æ›´å¥½
    demo = gr.Interface(
        fn=solve_problem,
        inputs=[
            gr.Textbox(
                label="è¾“å…¥æ•°å­¦é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šå°æ˜æœ‰5ä¸ªè‹¹æœï¼Œåƒäº†2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ",
                lines=3,
            ),
            gr.Dropdown(
                choices=mode_choices,
                value="cot",
                label="æ¨ç†æ¨¡å¼ (cot=é“¾å¼æ€ç»´, detailed=è¯¦ç»†è§£æ, standard=æ ‡å‡†, simple=ç®€æ´)",
            ),
            gr.Slider(
                minimum=0,
                maximum=1,
                value=0.1,
                step=0.1,
                label="æ¸©åº¦ (åˆ›é€ æ€§)",
            ),
        ],
        outputs=gr.Textbox(label="è§£ç­”ç»“æœ", lines=15),
        title="ğŸ§® å°å­¦æ•°å­¦æ¨ç†é—®ç­”ç³»ç»Ÿ",
        description="åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦é—®é¢˜è§£ç­”åŠ©æ‰‹ï¼Œæ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ã€‚",
        examples=[
            ["å°æ˜æœ‰12ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢5ä¸ªï¼Œåˆä¹°äº†8ä¸ªï¼Œç°åœ¨æœ‰å¤šå°‘ä¸ªï¼Ÿ", "cot", 0.1],
            ["ä¸€ä¸ªé•¿æ–¹å½¢çš„é•¿æ˜¯15ç±³ï¼Œå®½æ˜¯8ç±³ï¼Œæ±‚å‘¨é•¿å’Œé¢ç§¯ã€‚", "detailed", 0.1],
            ["ç”²ä¹™ä¸¤äººå…±æœ‰é’±240å…ƒï¼Œç”²çš„é’±æ•°æ˜¯ä¹™çš„2å€ï¼Œä¸¤äººå„æœ‰å¤šå°‘é’±ï¼Ÿ", "cot", 0.1],
        ],
        allow_flagging="never",
    )
    
    return demo


def main():
    parser = argparse.ArgumentParser(description="æ•°å­¦æ¨ç†é—®ç­”ç³»ç»Ÿ")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument("--lora_path", type=str, default=None)
    parser.add_argument("--mode", type=str, choices=["cli", "web"], default="cli",
                        help="è¿è¡Œæ¨¡å¼: cli=å‘½ä»¤è¡Œ, web=ç½‘é¡µç•Œé¢")
    parser.add_argument("--port", type=int, default=7860, help="Webç•Œé¢ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    
    args = parser.parse_args()
    
    assistant = MathAssistant(args.model, args.lora_path)
    
    if args.mode == "web":
        if not HAS_GRADIO:
            print("é”™è¯¯: éœ€è¦å®‰è£… gradio (pip install gradio)")
            return
        demo = create_gradio_interface(assistant)
        demo.launch(server_name="0.0.0.0", server_port=args.port, share=args.share)
    else:
        run_cli(assistant)


if __name__ == "__main__":
    main()